{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network method\n",
    "\n",
    "In DQN, the Q-learning algorithm is modified such that the Q-values are estimated using a neural network. The input to the neural network is the state and the output is the Q-values of all the actions at the state. This code has been written using the following tutorial as a baseline for further modifications: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c and closely following the pseudocode found here: https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the environment and set the goal\n",
    "from ipynb.fs.full.PianoHandv1 import *\n",
    "train_key = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "env = PianoHandEnv(train_key)\n",
    "\n",
    "state_space        = env.observation_space.shape[0]   # State space of the environment \n",
    "action_space       = env.action_space.n               # Action space of the environment \n",
    "hidden_layers      = 16                               # Hidden layers of neural network\n",
    "total_episodes     = 10\n",
    "max_steps          = 100\n",
    "sample_batch_size  = 32                               # Batch size for experience replay\n",
    "memory             = deque(maxlen=128)                # Memory for experience replay\n",
    "learning_rate      = 0.01                             # Learning rate - the amount of change to the model during each step         \n",
    "gamma              = 0.95                             # Discount rate - how important are the immediate rewards vs later rewards\n",
    "epsilon            = 0.9                              # Probability of exploring vs exploiting\n",
    "exploration_rate   = 1.0\n",
    "exploration_min    = 0.01\n",
    "exploration_decay  = 0.995\n",
    "reward_array       = []\n",
    "time_array         = []\n",
    "\n",
    "# This function builds the neural network model which consists of fully connected layers with the input as the states and \n",
    "# the outputs as the Q-values of different actions at the state.\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    inputs = Input(shape=(state_space,))\n",
    "    layer1 = Dense(hidden_layers, activation = 'relu')(inputs)\n",
    "    layer2 = Dense(action_space, activation = 'linear')(layer1)\n",
    "    model = keras.Model(inputs=inputs, outputs=layer2)\n",
    "    model.compile(loss='mse', optimizer=SGD(lr=learning_rate))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# This function generates the action chosen using the epsilon greedy policy.\n",
    "def e_greedy(state, model):\n",
    "    # If the random number generated is smaller than epsilon, choose to explore, otherwise take best action.\n",
    "    if np.random.uniform(0,1) < epsilon:          \n",
    "        action = env.action_space.sample()                     # Exploration \n",
    "    else:\n",
    "        action = np.argmax((model.predict(state))[0])          # Exploitation \n",
    "    return action\n",
    "\n",
    "def experience_replay(sample_batch_size):\n",
    "    sample_batch = random.sample(memory, sample_batch_size)                 # Sample random minibatch\n",
    "    for state, action, reward, next_state, done in sample_batch:\n",
    "        if done:\n",
    "            target = reward                                                 \n",
    "        if not done:\n",
    "            target = reward + gamma * np.amax(model.predict(next_state)[0])\n",
    "        \n",
    "        # Perform gradient descent \n",
    "        target_new = model.predict(state)                                     \n",
    "        target_new[0][action] = target\n",
    "        model.fit(state, target_new, epochs=1, verbose=0)\n",
    "\n",
    "        \n",
    "# Build model\n",
    "model = build_model(state_space, action_space)\n",
    "\n",
    "# Main loop to be run over all episodes\n",
    "for episode in range(total_episodes):\n",
    "    ep_start = time.time()\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_space])\n",
    "    done = False\n",
    "    index = 0\n",
    "    for step in range(max_steps):\n",
    "        action = e_greedy(state, model)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_space])\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        memory.append((state, action, reward, next_state, done))  \n",
    "        state = next_state\n",
    "        index += 1\n",
    "        reward_array.append(reward)\n",
    "        \n",
    "        # Experience replay to sample a random batch and update target\n",
    "        if len(memory) >= sample_batch_size:\n",
    "            experience_replay(sample_batch_size)\n",
    "        \n",
    "        # Decaying epsilon greedy\n",
    "        if exploration_rate > exploration_min:\n",
    "            exploration_rate *= exploration_decay\n",
    "    \n",
    "    # Keep track of time\n",
    "    ep_end = time.time()\n",
    "    time_diff = ep_end - ep_start\n",
    "    time_array.append(time_diff)\n",
    "    \n",
    "    print(\"Episode {} Average rewards = {}\". format(episode, np.mean(reward_array)), end = '\\r')\n",
    "\n",
    "# Save last best model\n",
    "# weight_path = \"./\" + train_key + \".h5\"\n",
    "# model.save_weights(weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suboptimal training times\n",
    "The training of the agent using this algorithm took very long hours, with sub-optimal results, even after reducing the state and action space, and tuning the parameters. Actor-Critic method produced better results with similar conditions, and hence, Actor-Critic was chosen as the algorithm to move forward with.\n",
    "The following code was used to plot the average time taken per episode for training a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  \n",
    "plt.plot(time_diff)\n",
    "plt.ylabel(\"Time taken per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.title(\"Time taken per episode vs episodes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
