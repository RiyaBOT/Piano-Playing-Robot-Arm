{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Method\n",
    "\n",
    "The Actor-critic method is a temporal-difference, on-policy deep reinforcement learning algorithm. The 'actor' attempts to learn the optimal policy and follow it, while the 'critic' critiques this policy by calculating the optimal value function and providing feedback. This produces a more efficient algorithm for training the agent. \n",
    "\n",
    "The code used for training the agent using actor-critic method on our custom environment has been developed by using the following tutorial here as a baseline for further modifications: https://keras.io/examples/rl/actor_critic_cartpole/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the environment and set the goal\n",
    "from ipynb.fs.full.PianoHandv1 import *\n",
    "train_key = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # Create the gym environment using the specific goal \n",
    "    env = PianoHandEnv(train_key)\n",
    "    eps = np.finfo(np.float32).eps.item() \n",
    "    \n",
    "    # Setting parameters\n",
    "    gamma           = 0.9                            # Discount rate - how important are the immediate rewards vs later rewards\n",
    "    learning_rate   = 0.1                            # Learning rate - the amount of change to the model during each step \n",
    "    state_space     = env.observation_space.shape[0] # State space of theenvironment \n",
    "    action_space    = env.action_space.n             # Action space of theenvironment \n",
    "    hidden_layers   = 2                              # Hidden layers of neural network\n",
    "    max_steps       = 50                             # Maximum steps the agent is allowed to take\n",
    "    total_episodes  = 100                            # Total episodes  \n",
    "    \n",
    "    # Initialization for running tensorboard\n",
    "    log_dir         = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    summary_writer  = tf.summary.create_file_writer(logdir=log_dir)\n",
    "    \n",
    "    # Initialising variables and arrays used in training\n",
    "    actor_history           = []\n",
    "    critic_history          = []\n",
    "    rewards_history         = []\n",
    "    rewards_in_all_episodes = []\n",
    "    train_loss_results      = []\n",
    "    train_rewards_results   = []\n",
    "\n",
    "    action_distribution = np.zeros(16)\n",
    "    done = False\n",
    "    \n",
    "    # The best optimizer determined is Root Mean Square Propagation (RMSProp) and the best loss function is Huber loss.\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate)    \n",
    "    loss      = keras.losses.Huber()   \n",
    "    \n",
    "    \"\"\"\n",
    "    Uncomment the following lines to use other optimizers/loss functions\n",
    "    optimizer = keras.optimizers.Adagrad(learning_rate=0.1)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "    loss = keras.losses.KLDivergence()\n",
    "    loss = keras.losses.SquaredHinge()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def create_model(self, state_space, action_space, hidden_layers):\n",
    "        '''\n",
    "        A shared neural network structure with dense (fully connected) layers is used to implement the actor critic network.\n",
    "        After testing with different number of hidden layers, the optimal number of layers is set as 2.\n",
    "        ''' \n",
    "        inputs = Input(shape=(state_space,))                          \n",
    "        common = Dense(hidden_layers, activation=\"relu\")(inputs)     # The first layer is common to both actor and critic\n",
    "        actor  = Dense(action_space, activation=\"softmax\")(common)   # Output for the actor - probabilities for each action at a state\n",
    "        critic = Dense(1)(common)                                    # Output for the critic - estimated total returns\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        model = self.create_model(self.state_space, self.action_space, self.hidden_layers)\n",
    "        count = 0\n",
    "        for episode in range (self.total_episodes):           \n",
    "            st = self.env.reset()\n",
    "            \n",
    "            #episode_reward = 0 \n",
    "            state = (st[0][0], st[0][1], st[1][0], st[1][1], st[2][0], st[2][1], st[3][0], st[3][1])\n",
    "            \n",
    "            #To demonstrate the how the losses and rewards change in realtime usinf TensorBoard\n",
    "            epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "            epoch_rewards_avg= tf.keras.metrics.Mean()\n",
    "            epoch_critic_loss_avg = tf.keras.metrics.Mean()\n",
    "            epoch_actor_loss_avg= tf.keras.metrics.Mean()\n",
    "        \n",
    "            #epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            \n",
    "            \"\"\"\n",
    "            Made use of GradientTape because of the automatic differentiation property\n",
    "            which is useful in the backpropogation of this algorithm for training the \n",
    "            neural network structure.\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(self.max_steps): \n",
    "                    \n",
    "                    state = tf.convert_to_tensor(state) #State is converted into the dimensions recognised by model\n",
    "                    state = tf.expand_dims(state, 0) \n",
    "\n",
    "                    action_probs, critic_value = model(state)\n",
    "                    self.critic_history.append(critic_value[0, 0])\n",
    "                    \n",
    "                    #choosing an action based on the action probability\n",
    "                    action = np.random.choice(self.action_space, p=np.squeeze(action_probs)) \n",
    "                    \n",
    "                    #updating the action probability based on the action choosen\n",
    "                    self.action_distribution[action]+=1 \n",
    "                    \n",
    "                    self.actor_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    st, reward, done, final, link = self.env.step(action)\n",
    "                    state = (st[0][0], st[0][1], st[1][0], st[1][1], st[2][0], st[2][1], st[3][0], st[3][1])\n",
    "                    self.rewards_history.append(reward)\n",
    "                    \n",
    "                    if done:\n",
    "                        # Render and save weights only for last episode\n",
    "                        #if episode == (self.total_epsiodes-1):\n",
    "                            #env.render(timestep, done)\n",
    "                            #weight_path = \"./\" + train_key + \".h5\"   \n",
    "                            #model.save_weights(weight_path)\n",
    "                        break\n",
    "                        \n",
    "                self.rewards_in_all_episodes.append(reward)\n",
    " \n",
    "                returns = []\n",
    "                ds = 0.0 #DiscountedSum\n",
    "        \n",
    "                \"\"\"\n",
    "                Calculating the returns at each time step. Starting from the \n",
    "                last time-step and going to the first, the rewards for each \n",
    "                time step are discounted by the factor of gamma.\n",
    "                \"\"\"\n",
    "                for r in self.rewards_history[::-1]:\n",
    "                    ds = r + self.gamma * ds\n",
    "                    returns.append(ds)\n",
    "    \n",
    "                returns = np.array(returns)\n",
    "                \n",
    "                #Since our reward structure ranges from -10 to 200, we normalized the returns.\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                history = zip(self.actor_history, self.critic_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "\n",
    "                for probabilityofaction, value, return_obtained in history:\n",
    "\n",
    "                    variation = return_obtained - value \n",
    "                    #ret is the reward obtained by the agent and value is the reward estimated\n",
    "                    #to be got by the critic.\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    The below action updates the actor so that it gives a higher probability of choosing \n",
    "                    to the actions which give a higher reward.\n",
    "                    \"\"\"\n",
    "                    actor_losses.append(-probabilityofaction * variation) \n",
    "                      \n",
    "                    \"\"\"\n",
    "                    The below action updates the critic so that it can better approximate the future returns.\n",
    "                    \"\"\"\n",
    "                    critic_losses.append(\n",
    "                        self.loss(tf.expand_dims(value, 0), tf.expand_dims(return_obtained, 0))\n",
    "                    )\n",
    "                    \n",
    "                #Here is where backpropogation occurs. Backpropogation calculates the\n",
    "                #gradient of the error function with respect to the neural network's weights.\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                \n",
    "                #Automatic Differentiation using Gradient Tape\n",
    "                grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "                \n",
    "                epoch_loss_avg(loss_value)\n",
    "                epoch_rewards_avg(reward)\n",
    "                epoch_critic_loss_avg(sum(critic_losses))\n",
    "                epoch_actor_loss_avg(sum(actor_losses))\n",
    "\n",
    "                self.train_loss_results.append(epoch_loss_avg.result()) \n",
    "                self.train_rewards_results.append(epoch_rewards_avg.result())\n",
    "\n",
    "                self.actor_history.clear()\n",
    "                self.critic_history.clear()\n",
    "                self.rewards_history.clear()\n",
    "                \n",
    "                count += 1\n",
    "               \n",
    "            if count % 10 == 0:\n",
    "                print(\"Episode {} Average rewards = {}\". format(count, np.mean(self.rewards_in_all_episodes)), end = '\\r')\n",
    "             \n",
    "            # Display results in tensorboard\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('epoch_loss_avg', epoch_loss_avg.result(), step=self.optimizer.iterations)\n",
    "                tf.summary.scalar('epoch_reward_avg', epoch_rewards_avg.result(), step= self.optimizer.iterations)\n",
    "                tf.summary.scalar('epoch_critic_loss_avg', epoch_critic_loss_avg.result(), step=self.optimizer.iterations)\n",
    "                tf.summary.scalar('epoch_actor_loss_avg', epoch_actor_loss_avg.result(), step= self.optimizer.iterations)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train agent\n",
    "agent = Agent()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard \n",
    "log_dir = \".\\logs\"\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
