{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of performance with other algorithms\n",
    "\n",
    "The performance of the deep learning agent is compared with a classical control algorithm (PID control) and a tabular reinforcement learning algorithm (SARSA) in order to determine how deep learning is better in the case of articulated robots with a large state space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.PianoHandv1 import *\n",
    "# Specify the goal (key) to test here. \n",
    "test_key = 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Control: Proportional-Integral-Derivative (PID) control\n",
    "\n",
    "In a PID controller, the actual value of the output is compared with the desired output and the current action adjusted based on the error between the two. The desired output signal is calculated by summing the individual responses of the proportional, integral and derivative components of the system.\n",
    "The following formula is used to calculated the output signal.\n",
    "$$ Output = K_p*e(t) + K_i*\\int_{0}^{t} e(t) \\,dt + K_d*\\frac{d}{dt}*(e(t)) $$\n",
    "\n",
    "The following code implements the PID control in our custom environment and assesses its performance. This code has been adapted from https://gist.github.com/HenryJia/23db12d61546054aa43f8dc587d9dc2c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "env = PianoHandEnv(test_key)\n",
    "desired_state = np.array([45, 45, 45, 45, 45, 45, 55, 45])\n",
    "desired_mask = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "action_distribution = np.zeros(15)\n",
    "total_episodes = 100\n",
    "\n",
    "# The parameters have been tuned to give us the best result\n",
    "P, I, D = 0.7, 0.003, 0.5\n",
    "inte = []\n",
    "der = []\n",
    "\n",
    "reward_count_pid = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    st = env.reset()\n",
    "    state = (st[0][0], st[0][1], st[1][0], st[1][1], st[2][0], st[2][1], st[3][0], st[3][1])\n",
    "    prev_error = 0\n",
    "         \n",
    "    for t in range(500):\n",
    "        error = state - desired_state\n",
    "        integral = 0\n",
    "        derivative = 0\n",
    "       \n",
    "        # Calculating the output using the PID control formula\n",
    "        integral += error\n",
    "        derivative = error - prev_error\n",
    "        prev_error = error\n",
    "        inte.append(integral)\n",
    "        der.append(derivative)\n",
    "\n",
    "        pid = np.dot(P * error + I * integral + D * derivative, desired_mask)\n",
    "        action = sigmoid(pid)\n",
    "        \n",
    "        # Since a sigmoid function is used, the actions need to be normalized to the range [0,15]\n",
    "        action = action * 15\n",
    "        action = np.round(action).astype(np.int32)\n",
    "        \n",
    "        action_distribution[action]= action_distribution[action] + 1\n",
    "        st, reward, done, info, final = env.step(action)\n",
    "        reward_count_pid.append(reward)\n",
    "        state = (st[0][0], st[0][1], st[1][0], st[1][1], st[2][0], st[2][1], st[3][0], st[3][1])\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(\"Episode {} Average rewards = {}\". format(episode+1, np.mean(reward_count_pid)), end = '\\r')\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results obtained from PID control\n",
    "The following graphs display the moving average of rewards earned over all episodes and the action distrubution curve which shows the frequency of choosing an action using this algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_rewards(rewards, num_of_episodes, partition_parameter):\n",
    "    rewards_per_partition = np.split(np.array(rewards), num_of_episodes/partition_parameter)\n",
    "    average_rewards_per_partition=[]\n",
    "    episode_number=[]\n",
    "    counter=0\n",
    "    \n",
    "    for r in rewards_per_partition: #calculating the average reward for each partition\n",
    "        average_rewards_per_partition.append(sum(r/partition_parameter))\n",
    "        episode_number.append(counter)\n",
    "        counter+= partition_parameter\n",
    "        \n",
    "    #Design of the plot\n",
    "    plt.figure()\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"   \n",
    "    plt.plot(episode_number, average_rewards_per_partition, color='black')\n",
    "    plt.ylabel(\"Moving average of total reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.title(\"Moving Average of Total Reward per Episode\")\n",
    "    plt.grid('on')\n",
    "\n",
    "moving_average_rewards(reward_count_pid, total_episodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the action distribution\n",
    "plt.plot(action_distribution, color = 'black')\n",
    "plt.grid('on')\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"Number of Times Action was Executed\")\n",
    "plt.title(\"Action Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Reinforcement learning: SARSA \n",
    "\n",
    "SARSA is a model free method, on-policy method of approximating the action value function to obtain the optimal policy, using the Temporal Difference (TD) Methods. It is a tabular method where the Q-values are stored in a look-up table, which is used to pick the best action at any given state. The Q-values are calculated using the following equation:\n",
    "$$Q(S_{t},A_{t})\\leftarrow Q(S_{t},A_{t}) + \\alpha*[R_{t+1} + \\gamma*Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t})]$$\n",
    "\n",
    "Since our state space is quite large ($31^8$ states), this tabular method is not feasible for training the model. In this code, we have demonstrated the use of SARSA to train only one finger of the robotic hand, which reduces the state space to $31^2$ states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "alpha             = 0.4           # Learning rate - rate at which Q values are updated\n",
    "gamma             = 0.9           # Discount rate - how important are the immediate rewards vs later rewards\n",
    "epsilon_normal    = 0.01          # Used to balance exploring versus exploiting\n",
    "total_episodes    = 100           # Total episodes for training\n",
    "\n",
    "# For decaying epsilon greedy\n",
    "decay             = True          # Change to True for implementing decay\n",
    "epsilon_decay     = 0.95          # Initial epsilon value\n",
    "min_epsilon       = 0.01          # Minimum value to decay until  \n",
    "decay_rate        = 0.01          # Decay rate\n",
    "\n",
    "def e_greedy(state, Q, epsilon):\n",
    "    # If the random number generated is smaller than epsilon, choose to explore, otherwise take best action.\n",
    "    if np.random.uniform(0,1) < epsilon:          \n",
    "        action = random.randint(12, 15)           # Exploration \n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])           # Exploitation \n",
    "    return action\n",
    "\n",
    "def SARSA(env):\n",
    "    \n",
    "    # Initialise Q-table\n",
    "    states = 50*50\n",
    "    Q = np.zeros((states, 16))\n",
    "    reward_count = []\n",
    "    \n",
    "    # For decaying epsilon greedy\n",
    "    if (decay == True):\n",
    "        epsilon = epsilon_decay\n",
    "    else:\n",
    "        epsilon = epsilon_normal\n",
    "    \n",
    "    # Main Loop\n",
    "    for episode in range(total_episodes):\n",
    "        st = env.reset()\n",
    "        \n",
    "        # Reshape the state so that it can be used in a 2D Q-table\n",
    "        state = (st[0][0], st[0][1], st[1][0], st[1][1], st[2][0], st[2][1], st[3][0], st[3][1])\n",
    "        theta1 = st[3][0]-30\n",
    "        theta2 = st[3][1]-30\n",
    "        state = theta1*30 + theta2\n",
    "        \n",
    "        #print(state)\n",
    "        #if (episode % 1 == 0):\n",
    "            #print(\"Episode {}\".format(episode))\n",
    "            \n",
    "        step = 0                                                \n",
    "        action = e_greedy(state, Q, epsilon)                    \n",
    "        done = False\n",
    "\n",
    "        # The following while loop illustrates one episode.\n",
    "        while step < 50:\n",
    "\n",
    "            ns, reward, done, _, _ = env.step(action)   # Take a step and observe results\n",
    "            \n",
    "            # Reshape the next state to be updated in 2D Q-table\n",
    "            ntheta1 = ns[0][0]-30\n",
    "            ntheta2 = ns[0][1]-30\n",
    "            next_state= ntheta1*30 + ntheta2\n",
    "            next_action = e_greedy(next_state, Q, epsilon)      \n",
    "            \n",
    "            # SARSA update rule\n",
    "            predicted_value = Q[state, action]                \n",
    "            target_value = reward + (gamma * Q[next_state, next_action])\n",
    "            Q[state, action] += alpha * (target_value - predicted_value)\n",
    "            \n",
    "            state  = next_state\n",
    "            action = next_action\n",
    "            step   += 1 \n",
    "            \n",
    "            # Decaying epsilon greedy formula\n",
    "            if decay == True:\n",
    "                if (epsilon > min_epsilon):\n",
    "                    epsilon = epsilon*(1-decay_rate)**step \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        reward_count.append(reward)\n",
    "        print(\"Episode {} Average rewards = {}\". format(episode+1, np.mean(reward_count)), end = '\\r')\n",
    "        time.sleep(0.25)\n",
    "    \n",
    "    return (Q, reward_count)\n",
    "\n",
    "env = PianoHandEnv(test_key)\n",
    "Q_new, reward_count = SARSA(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results obtained using SARSA\n",
    "\n",
    "Plotting the moving average of rewards earned over all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_rewards(reward_count, total_episodes, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
